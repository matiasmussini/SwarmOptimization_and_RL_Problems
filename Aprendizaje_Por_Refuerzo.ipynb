{"cells":[{"cell_type":"markdown","metadata":{"id":"tbWRIR_5dP_K"},"source":["# Gridworld y Jack’s Car Rental.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"JkSwBufDdP_O"},"source":["### Gridworld"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy as np\n","import warnings "]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["warnings.filterwarnings(\"ignore\")\n","\n","def Gridworld(rejilla, recompensa, gamma, epsilon):\n","    \"\"\"\n","    Método que devuelve las recompensas y política con la mayor recompensa desde cualquier casilla.\n","    :param rejilla: Rejilla con la información de cada casilla.\n","    :param recompensa: Recompensa por cada movimiento, en nuestro caso será siempre -1.\n","    :param gamma: Factor de descuento.\n","    :param epsilon: Umbral de convergencia.\n","\n","    :return: Recompensas con la mayor recompensa desde cualquier casilla y política con la acción que debe tomarse desde cada casilla.\n","\n","    \"\"\"\n","\n","    # Matrices de recompensas y política inicializadas a 0 y vacío respectivamente\n","    recompensas = np.zeros((len(rejilla), len(rejilla[0])))\n","    politica = np.full((len(rejilla), len(rejilla[0])), np.nan, dtype=object)\n","    operar = True \n","    while operar:\n","        # Inicializamos la diferencia actual entre las iteraciones\n","        diferencia_actual = 0\n","\n","        # Iteramos sobre cada casilla de la rejilla\n","        for i in range(len(rejilla)):\n","            for j in range(len(rejilla[0])):\n","                \n","                # Ignoramos casillas con T\n","                if rejilla[i][j] == 'T':\n","                    continue\n","\n","                # Casillas sin información para el caso b)\n","                if rejilla[i][j] is None:\n","                    recompensas[i][j] = np.nan\n","                    politica[i][j] = np.nan\n","                    continue\n","\n","                # Casilla apartado b) caso especial\n","                if rejilla[i][j] == 15:\n","                    # Almacenamos el valor actual de la casilla\n","                    valor_ant = recompensas[i][j]\n","\n","                    # Definimos los vecinos de la casilla a mano ya que es un caso especial\n","                    vecinos = [[3, 1], [4, 1], [3, 0], [3, 2]]\n","\n","                    # Inicializamos variables para el máximo valor y la acción correspondiente\n","                    mayor_val = float('-inf')\n","                    max_mov = np.nan\n","\n","                    # Iteramos sobre los vecinos para encontrar la acción óptima (en este caso solo hay uno)\n","                    for vecino in vecinos:\n","                        if vecino[0] < 0 or vecino[0] >= len(rejilla) or vecino[1] < 0 or vecino[1] >= len(rejilla[0]):\n","                            continue\n","                        valor = recompensa + gamma * recompensas[vecino[0]][vecino[1]]\n","                        if valor > mayor_val:\n","                            mayor_val = valor\n","                            max_mov = vecino\n","\n","                    # Actualizamos la recompensa y la política (en este caso solo hay una acción)\n","                    recompensas[i][j] = mayor_val\n","                    diferencia_actual = max(diferencia_actual, abs(valor_ant - recompensas[i][j]))\n","                    politica[i][j] = 'diagonal \"arriba y derecha\"'\n","\n","                # Resto de las casillas \n","                else:\n","                    # Almacenamos el valor actual de la casilla \n","                    valor_ant = recompensas[i][j]\n","\n","                    # Definimos los vecinos de la casilla actual \n","                    vecinos = [[i-1, j], [i+1, j], [i, j-1], [i, j+1]]\n","\n","                    # Inicializamos variables para el máximo valor y la acción correspondiente \n","                    mayor_val = float('-inf')\n","                    max_mov = np.nan\n","\n","                    # Iteramos sobre los vecinos para encontrar la acción óptima \n","                    for vecino in vecinos:\n","                        if vecino[0] < 0 or vecino[0] >= len(rejilla) or vecino[1] < 0 or vecino[1] >= len(rejilla[0]):\n","                            continue\n","                        valor = recompensa + gamma * recompensas[vecino[0]][vecino[1]]\n","                        if valor > mayor_val:\n","                            mayor_val = valor\n","                            #max_mov = vecino\n","\n","                    max_mov = []\n","\n","                    for vecino in vecinos:\n","                        if vecino[0] < 0 or vecino[0] >= len(rejilla) or vecino[1] < 0 or vecino[1] >= len(rejilla[0]):\n","                            continue\n","                        valor = recompensa + gamma * recompensas[vecino[0]][vecino[1]]\n","                        if valor == mayor_val:\n","                            max_mov.append(vecino)\n","\n","\n","                    # Actualizamos la recompensa y la política \n","                    recompensas[i][j] = mayor_val\n","                    diferencia_actual = max(diferencia_actual, abs(valor_ant - recompensas[i][j]))\n","\n","                    # Determinamos la acción óptima según la dirección del vecino con mayor valor\n","                    aux = []\n","                    for z in max_mov:\n","                        \n","                        if i == z[0] and j - 1 == z[1]:\n","                            aux.append('izquierda')\n","                        elif i == z[0] and j + 1 == z[1]:\n","                            aux.append('derecha')\n","                        elif i - 1 == z[0] and j == z[1]:\n","                            aux.append('arriba')\n","                        elif i + 1 == z[0] and j == z[1]:\n","                            aux.append('abajo')\n","\n","                    politica[i][j] = aux\n","\n","        # Comprobamos si la diferencia actual es menor que el umbral epsilon para acabar el proceso si lo alncanzamos\n","        if diferencia_actual < epsilon:\n","            operar = False\n","\n","    # Devolvemos las recompensas y la política finales\n","    return recompensas, politica\n"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[],"source":["def print_policy(policy_matrix):\n","    for row in policy_matrix:\n","        for cell in row:\n","            if cell == \"nan\":\n","                print(\"{: <50}\".format(\"nan\"), end=\" \")\n","            else:\n","                if isinstance(cell, list):\n","                    movements = \", \".join(cell)\n","                else:\n","                    movements = cell\n","                print(\"{: <50}\".format(f\"[{movements}]\"), end=\" \")\n","        print(\"\\n\")"]},{"cell_type":"markdown","metadata":{"id":"v3351DmsdP_R"},"source":["#### Resultados"]},{"cell_type":"markdown","metadata":{"id":"F4M0GoFBdP_R"},"source":["a)"]},{"cell_type":"markdown","metadata":{},"source":["Datos"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["rejilla = [['T', 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 'T']]\n","recompensa = -1\n","gamma = 1\n","epsilon = 1e-6"]},{"cell_type":"code","execution_count":28,"metadata":{"id":"35xhrtxddP_R","outputId":"77d4565d-184e-4e20-9ea4-56bd9fb83674"},"outputs":[{"name":"stdout","output_type":"stream","text":["Matriz de recompensas: \n","\n","[[ 0. -1. -2. -3.]\n"," [-1. -2. -3. -2.]\n"," [-2. -3. -2. -1.]\n"," [-3. -2. -1.  0.]]\n","\n","\n","Politica de movimientos:\n","\n","\n","[nan]                                              [izquierda]                                        [izquierda]                                        [abajo, izquierda]                                 \n","\n","[arriba]                                           [arriba, izquierda]                                [arriba, abajo, izquierda, derecha]                [abajo]                                            \n","\n","[arriba]                                           [arriba, abajo, izquierda, derecha]                [abajo, derecha]                                   [abajo]                                            \n","\n","[arriba, derecha]                                  [derecha]                                          [derecha]                                          [nan]                                              \n","\n"]}],"source":["recompensas, politica = Gridworld(rejilla, recompensa, gamma, epsilon)\n","print(f'Matriz de recompensas: \\n\\n{np.array(recompensas)}')\n","print('\\n')\n","print(f'Politica de movimientos:')\n","print('\\n')\n","print_policy(politica)"]},{"cell_type":"markdown","metadata":{"id":"JtsxSpZMdP_S"},"source":["b)"]},{"cell_type":"markdown","metadata":{},"source":["Datos"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gqrIc3ULIhGT","outputId":"d51fef76-afd7-4f99-e1a7-6153460dd70f"},"outputs":[],"source":["rejilla = [['T', 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 'T'],[None, 15, None, None]]\n","recompensa = -1\n","gamma = 1\n","epsilon = 1e-6"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Matriz de recompensas: \n","\n","[[ 0. -1. -2. -3.]\n"," [-1. -2. -3. -2.]\n"," [-2. -3. -2. -1.]\n"," [-3. -2. -1.  0.]\n"," [nan -2. nan nan]]\n","\n","\n","Politica de movimientos:\n","\n","\n","[nan]                                              [izquierda]                                        [izquierda]                                        [abajo, izquierda]                                 \n","\n","[arriba]                                           [arriba, izquierda]                                [arriba, abajo, izquierda, derecha]                [abajo]                                            \n","\n","[arriba]                                           [arriba, abajo, izquierda, derecha]                [abajo, derecha]                                   [abajo]                                            \n","\n","[arriba, derecha]                                  [derecha]                                          [derecha]                                          [nan]                                              \n","\n","[nan]                                              [diagonal \"arriba y derecha\"]                      [nan]                                              [nan]                                              \n","\n"]}],"source":["recompensas, politica = Gridworld(rejilla, recompensa, gamma, epsilon)\n","print(f'Matriz de recompensas: \\n\\n{np.array(recompensas)}')\n","print('\\n')\n","print(f'Politica de movimientos:')\n","print('\\n')\n","print_policy(politica)"]},{"cell_type":"markdown","metadata":{"id":"buKmz1ejdP_T"},"source":["---\n","\n","\n","### Jack's Car Rental"]},{"cell_type":"markdown","metadata":{"id":"bu1fL99TdP_U"},"source":["Librerias."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: scipy in c:\\users\\mussi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (1.7.1)\n","Requirement already satisfied: numpy<1.23.0,>=1.16.5 in c:\\users\\mussi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages (from scipy) (1.19.5)\n"]},{"name":"stderr","output_type":"stream","text":["WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mussi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n","WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mussi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n","WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mussi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n","WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mussi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n","WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mussi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n","WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\mussi\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.9_qbz5n2kfra8p0\\localcache\\local-packages\\python39\\site-packages)\n","\n","[notice] A new release of pip available: 22.3.1 -> 23.3.2\n","[notice] To update, run: C:\\Users\\mussi\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"]}],"source":["!pip install scipy "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UCGWtfqZMP3w","outputId":"d9d35102-a3a2-4a06-852c-467dddb408d6"},"outputs":[],"source":["from scipy.stats import poisson\n","import numpy as np"]},{"cell_type":"markdown","metadata":{"id":"7BD7LOn6dP_V"},"source":["#### Datos"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["#lambdas dados por el enunciado:\n","lambda_alquiler1 = 3\n","lambda_alquiler2 = 4\n","lambda_devolucion1 = 3\n","lambda_devolucion2 = 2\n","max_coches = 10 #realizamos prueba con 10 coches\n","max_traslado = 5 #máximo de prestamos de una tienda a otra en un día\n","factor_descuento = 0.9\n","abono = 10 #recompensa por un alquiler\n","coste_traslado = 2 #coste por coche trasladado\n","umbral = 20  #diferencia de rendimientos necesaria para parar\n","limite_superior = 6  # Para limitar el cálculo de probabilidades de Poisson ( al hacerlo con 10 lo dejamos al máximo)\n","mejorar = True  # Determina si seguir mejorando la política\n","iterar = True  # Evaluar si es necesario iterar otra vez\n","politica = np.zeros((max_coches + 1, max_coches + 1))  # Política\n","recom = np.zeros((max_coches + 1, max_coches + 1))  # Recompensa esperado\n","nuevo_recom = np.zeros((max_coches + 1, max_coches + 1))  # Valores de recompensa esperado nuevos\n","estados = [(i, j) for i in range(max_coches + 1) for j in range(max_coches + 1)]\n","acciones = np.arange(-max_traslado, max_traslado + 1)  # Las acciones posibles; expresadas como el número de autos trasladados de la localización 1 a la 2. Un número negativo indica que se han movido autos de la localización 2 a la 1.\n"]},{"cell_type":"markdown","metadata":{},"source":["### ALGORITMO"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Iteración 1 - Política:\n"," [[ 0.  0.  0.  0.  0.  0.  0. -3. -3. -4. -4.]\n"," [ 0.  0.  0.  0.  0.  0. -2. -2. -3. -3. -4.]\n"," [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n"," [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n"," [ 0.  0.  0.  0.  0.  0.  0. -1. -1. -2. -2.]\n"," [ 0.  0.  1.  1.  1.  0.  0.  0. -1. -1. -1.]\n"," [ 0.  2.  2.  2.  1.  1.  0.  0.  0.  0. -1.]\n"," [ 0.  3.  3.  2.  2.  1.  1.  0.  0.  0.  0.]\n"," [ 4.  4.  3.  3.  2.  2.  1.  1.  0.  0.  0.]\n"," [ 5.  4.  4.  3.  3.  2.  2.  1.  1.  0.  0.]\n"," [ 5.  5.  4.  4.  3.  3.  2.  2.  1.  0.  0.]]\n","\n","Iteración 2 - Política:\n"," [[ 0.  0.  0.  0.  0.  0.  0. -3. -3. -4. -4.]\n"," [ 0.  0.  0.  0.  0. -1. -2. -2. -3. -3. -4.]\n"," [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n"," [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n"," [ 0.  0.  0.  0.  0.  0.  0. -1. -1. -2. -2.]\n"," [ 0.  0.  1.  1.  1.  0.  0.  0. -1. -1. -1.]\n"," [ 0.  2.  2.  2.  1.  1.  1.  0.  0.  0. -1.]\n"," [ 3.  3.  3.  2.  2.  2.  1.  1.  0.  0.  0.]\n"," [ 4.  4.  3.  3.  3.  2.  2.  1.  1.  0.  0.]\n"," [ 5.  4.  4.  4.  3.  3.  2.  2.  1.  1.  0.]\n"," [ 5.  5.  5.  4.  4.  3.  3.  2.  2.  1.  0.]]\n","\n","Iteración 3 - Política:\n"," [[ 0.  0.  0.  0.  0.  0. -2. -3. -3. -4. -4.]\n"," [ 0.  0.  0.  0.  0. -1. -2. -2. -3. -3. -4.]\n"," [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n"," [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n"," [ 0.  0.  0.  0.  0.  0.  0. -1. -1. -2. -2.]\n"," [ 0.  1.  1.  1.  1.  0.  0.  0. -1. -1. -2.]\n"," [ 0.  2.  2.  2.  1.  1.  0.  0.  0. -1. -1.]\n"," [ 3.  3.  3.  2.  2.  1.  1.  1.  0.  0.  0.]\n"," [ 4.  4.  3.  3.  2.  2.  2.  1.  1.  0.  0.]\n"," [ 5.  4.  4.  3.  3.  3.  2.  2.  1.  1.  0.]\n"," [ 5.  5.  4.  4.  4.  3.  3.  2.  2.  1.  0.]]\n","\n","Iteración 4 - Política:\n"," [[ 0.  0.  0.  0.  0.  0. -2. -3. -3. -4. -4.]\n"," [ 0.  0.  0.  0.  0. -1. -2. -2. -3. -3. -4.]\n"," [ 0.  0.  0.  0.  0. -1. -1. -2. -2. -3. -3.]\n"," [ 0.  0.  0.  0.  0.  0. -1. -1. -2. -2. -3.]\n"," [ 0.  0.  1.  0.  0.  0.  0. -1. -1. -2. -2.]\n"," [ 0.  2.  1.  1.  1.  0.  0.  0. -1. -1. -2.]\n"," [ 0.  2.  2.  2.  1.  1.  0.  0.  0. -1. -1.]\n"," [ 3.  3.  3.  2.  2.  1.  1.  1.  0.  0.  0.]\n"," [ 4.  4.  3.  3.  2.  2.  2.  1.  1.  0.  0.]\n"," [ 5.  4.  4.  3.  3.  3.  2.  2.  1.  1.  0.]\n"," [ 5.  5.  4.  4.  4.  3.  3.  2.  2.  1.  0.]]\n","\n"]}],"source":["num_iteracion = 0\n","\n","# Función de costo por traslado de autos\n","def costo_traslado(accion):\n","    return -2 * abs(accion)\n","\n","# Función de recompensa por alquiler de autos\n","def recompensa_alquiler(num_alquilados):\n","    return 10 * num_alquilados\n","\n","# Función para calcular el rendimiento\n","def rendimiento(tienda, accion, recom):\n","    # Inicializamos los retornos con el costo de traslado asociado a la acción\n","    beneficios = costo_traslado(accion)\n","\n","    # Calculamos las nuevas ubicaciones de las tiendas después de la acción\n","    tienda1 = int(min(tienda[0] - accion, max_coches))\n","    tienda2 = int(min(tienda[1] + accion, max_coches))\n","\n","\n","    # Bucles anidados para las posibles cantidades de compradores en ambas tiendas\n","    for compradores_tienda1 in range(0, tienda1):\n","        for compradores_tienda2 in range(0, tienda2):\n","            # Calculamos la probabilidad de alquiler en ambas tiendas\n","            probabilidad_alquilar = poisson.pmf(compradores_tienda1, lambda_alquiler1) * poisson.pmf(compradores_tienda2, lambda_alquiler2)\n","\n","            # Determinamos la cantidad real de alquileres en cada tienda, los que realmente tenemos disponibles para alquilar\n","            alquiler_tienda1 = min(tienda1, compradores_tienda1)\n","            alquiler_tienda2 = min(tienda2, compradores_tienda2)\n","\n","            # Calculamos las recompensas por alquiler en ambas tiendas, con los coches realmente disponibles para alquilar\n","            recompensas = recompensa_alquiler(alquiler_tienda1) + recompensa_alquiler(alquiler_tienda2)\n","\n","            # Bucles anidados para las posibles cantidades de devoluciones en ambas tiendas\n","            for devoluciones_tienda1 in range(0, max_coches):\n","                for devoluciones_tienda2 in range(0, max_coches):\n","                    # Calculamos la probabilidad de devolución en ambas tiendas\n","                    probabilidad_devolucion = poisson.pmf(devoluciones_tienda1, lambda_devolucion1) * poisson.pmf(devoluciones_tienda2, lambda_devolucion2)\n","                    # Calculamos la probabilidad total del día\n","                    probabilidad_dia = probabilidad_alquilar * probabilidad_devolucion\n","\n","                    # Actualizamos las ubicaciones de las tiendas después de las devoluciones, teniendo en cuenta que no nos podemos pasar del máximo de coches.\n","                    tienda1_ = min(tienda1 - alquiler_tienda1 + devoluciones_tienda1, max_coches)\n","                    tienda2_ = min(tienda2 - alquiler_tienda2 + devoluciones_tienda2, max_coches)\n","\n","                    # Actualizamos los retornos considerando la probabilidad del día y el factor de descuento\n","                    beneficios += probabilidad_dia * (recompensas + factor_descuento * recom[tienda1_, tienda2_])\n","    \n","    # Devolvemos el rendimiento total considerando todas las posibilidades\n","    return beneficios\n","\n","# Función para evaluar la política\n","def evaluar_politica():\n","    nuevo_recom = np.zeros((max_coches + 1, max_coches + 1))\n","    for i, j in estados:\n","        nuevo_recom[i, j] = rendimiento([i, j], politica[i, j], recom)\n","    return nuevo_recom\n","\n","# Función para mejorar la política\n","def mejorar_politica():\n","    nueva_politica = np.zeros((max_coches + 1, max_coches + 1))\n","    for i, j in estados:\n","        recompensas = []\n","        for accion in acciones:\n","            # Verificamos si la acción es válida dadas las restricciones\n","            if ((accion >= 0 and i >= accion) or (accion < 0 and j >= np.abs(accion))):\n","                # Calculamos la recompensa para cada acción válida\n","                recompensas.append(rendimiento([i, j], accion, recom))\n","            else:\n","                # Asignamos un valor negativo infinito a acciones inválidas\n","                recompensas.append(float('-inf'))\n","        # Seleccionamos la acción que maximiza la recompensa\n","        mejorAccion = np.argmax(recompensas)\n","        nueva_politica[i, j] = acciones[mejorAccion]\n","    return nueva_politica\n","\n","while True:\n","    num_iteracion += 1\n","\n","    # Evaluación de política\n","    nuevo_recom = evaluar_politica()\n","\n","    if np.sum(np.abs(nuevo_recom - recom)) < umbral:\n","        break\n","\n","    # Mejora de política\n","    nueva_politica = mejorar_politica()\n","\n","    # Comprobar si la política ha cambiado\n","    if np.array_equal(nueva_politica, politica):\n","        break\n","\n","    politica = nueva_politica\n","    recom = nuevo_recom\n","\n","    print(f\"Iteración {num_iteracion} - Política:\\n {politica}\\n\")\n","    #print(f\"Iteración {num_iteracion} - recom: {nuevo_recom}\\n\")\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"}}},"nbformat":4,"nbformat_minor":0}
